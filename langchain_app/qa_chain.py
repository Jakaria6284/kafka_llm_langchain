from langchain_groq import ChatGroq  # Try this import
from langchain_community.vectorstores import FAISS
from config.env import GROK_API_KEY
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
import requests

def load_qa_chain():
    embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    db = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)

    retriever = db.as_retriever()

    llm = ChatGroq(  # Use ChatGroq instead of Groq
        model='meta-llama/llama-4-scout-17b-16e-instruct',
        api_key=GROK_API_KEY
    )
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type='stuff',
        retriever=retriever,
        return_source_documents=True
    )
    return chain

def answer_question(question):
    print(f"\nüîç Searching context for: {question}")
    chain = load_qa_chain()

    # Call the chain, get the full dict output
    output = chain.invoke({"query": question})
    
    # Extract the answer text
    answer = output.get("result", "No answer found.")
    
    print(f"ü§ñ Answer generated by LLM: {answer}")
    return answer
